# WithMe开发笔记

## 2019.05.20
目前其实大部分基本工作已经做完了：
1. 前端仍然决定沿用之前的，不做太大的修改。
2. 使用微服务设计思想，服务之间通信使用resttemplate http。
3. 服务器使用Netty + WebSocket

前几天在思考Netty多实例的问题，后来发现还是想的太多了，先把单实例的搭起来调好再说吧。目前准备做的工作是：

1. 实现聊天记录的存储以及查询。
2. 实现离线消息的存储以及查询。
3. 实现群组聊天功能。

第一个问题我其实在纠结，刚开始想的是聊天记录先存redis，然后redis过期时再转存mysql，后来发现这样其实不太好，查了查也都没有这样做的，一般都是用redis当做缓存。后来想想确实还是只用redis当缓存的方式比较好，转存后查询起来确实麻烦。但是插入时就需要使用mysql了，那么像之前一样注入bean的方式当然是不能用了，能想到的最简单的方式是直接发送http请求，但是http请求比较耗时，因为是即时通讯从性能来考虑最好是发异步http请求，异步http又想到是不是可以放到mq里面，但是我对mq实在是不了解。过了一会儿我又想到既然http可以那rpc是不是也可以，然后翻了翻原来整理的使用rpc的demo，发现之前用的双方都是spring boot，这一方是spring boot一方是普通Java的还不太好办，使用dubbo吧发现还得用zookeeper，想想还是算了吧，后面再说。目前先用简单的http来实现吧。  
关于发送http请求之前我就简单查过，决定选用okhttp来发送，不是不想用RestTemplate，只是因为Netty服务器没有Spring的包，想使用有些麻烦，不如直接使用okhttp，正好很久之前就想试着用一下了。

## 2019.05.21
昨天查了一下，最后还是没用使用okhttp，因为它用起来貌似有些麻烦，后来是准备用RestTemplate的，它在非Spring项目中使用倒也不麻烦，只是需要导入spring-web这个包，直接结果就是让websocket-server-service 的jar包大小翻了一倍。。。仅仅为了一个函数就导入一个99%代码用不上的第三方jar包，唉。。。  
后来想了一想，为了避免插入数据库耗时过长影响即时聊天的时间，采用了异步的方式来发送插入聊天记录的http请求似乎更好一点，所以我又查了一下AsyncRestTemplate发现它过时了，推荐的替代方案是位于spring-webflux包下的WebClient，那就用呗，尝试着简单用了一下，调用成功了，不过目前是直接调用block，其实还是类似于同步的，后面等功能做的差不多时专门对比下同步和异步请求时间差多少。  
聊天记录成功插入之后就是聊天记录的查询了，今天调试修改前段和message-service后功能大概也跑通了，之前写的前端处理聊天消息的代码还是蛮好用的2333，小bug基本都出现在新写的后台。下面总结一下今天遇到的问题和知识点：  
1. spring-data-jpa 可以使用分页查询，具体做法是传入一个Pageable类型的参数，在查询之前使用PageRequest.of()新建Pageable对象，设置查询的页数、每页有多少条数据以及排序规则。
```java
Pageable sortedByTime = PageRequest.of(page, number, Sort.by("time"));
List<Message> messages = messageRepository.findAllByFromIdAndToIdOrFromIdAndToId(userIdA, userIdB, userIdB, userIdA, sortedByTime);
```
2. spring-data-jpa 默认可以解析or并且允许查询变量重复出现，如：
```java
/**
 * 查询两个用户之间的聊天记录
 *
 * @param fromId 用户A ID
 * @param toId 用户B ID
 * @param fromId2 用户B ID
 * @param toId2 用户A ID
 * @param pageable 分页对象，用户分页和排序
 * @return
 */
  List<Message> findAllByFromIdAndToIdOrFromIdAndToId(Integer fromId, Integer toId, Integer fromId2, Integer toId2, Pageable pageable);
```
这种查询也是支持的，只是参数传两遍貌似不太优雅，不过也想不到太好的解决方法。  
到目前为止正常实现的功能呢：
1. 登录/注册；
2. 双人聊天聊天记录的存储以及查询；
3. 聊天记录分页查询。
有待优化和实现的地方：
1. 聊天记录查询使用redis缓存;
2. 前端在顶部添加查看全部聊天记录按钮，弹出新窗口显示所有聊天记录。
第一点没有实现的原因是我想到：聊天记录是实时更新的，因为随时会有新的记录插入进去，那么这种情况下用redis做缓存意义是什么，几乎每次都要去查询新的结果，这种情况似乎不太适合吧。但是直接用mysql感觉又不太好，所以现在索性就先这样，后面等压测调优时找瓶颈，找到这里的话那就再改。  
第二点是属于功能上的完善，要加一些前端的内容，后面再加，先把几个基础功能先恢复了再说。

接下来应该实现的是：
1. 群组消息的转发。
2. 群组消息的存储以及查询（分页，优化结构）。
3. 创建群组、邀请人加群、查看群组成员功能的完善。
4. 离线消息的存储和查询、转发。

后面加油，慢慢做吧。

## 2019.06.19
这两天又开始继续写了，昨天把前端main.js又拆了一下，分成了两三个js，但是又遇到一个问题：ajax发送的获取单人和群组之间的聊天记录的请求一直404，昨晚看了一段时间没有找到解决办法，今天继续看一下。  
现在可以确认的情况是：
1. js请求路径没有问题。
2. 后台Api使用swagger测试之后也正常。
目前猜测有可能是nginx中间转发代理的路径不对。  

果然，经过尝试之后，果然是nginx location代理的问题，产生问题的原因为：  
我原来的location配置如下：
```
location ~ /v1/group {
    proxy_pass      http://group-service:8081;
}

location ~ /v1/groupMessages {
      proxy_pass      http://group-message-service:8081;
}
```
~ 是区分大小写的正则匹配，这也就意味着groupMessage也会被group匹配到，所以后面那条匹配是无效的，映射的服务地址错误了自然就报404了，修改一下顺序就可以了：
```
location ~ /v1/groupMessages {
      proxy_pass      http://group-message-service:8081;
}

location ~ /v1/group {
    proxy_pass      http://group-service:8081;
}
```
改正之后就可以了，以后也要注意这个问题。

接下来发送群组聊天消息，首先遇到一个问题：获取用户所有群组时遇到一下报错：
```
com.alibaba.fastjson.JSONException: write javaBean error, fastjson version 1.2.54, class cn.icedsoul.groupservice.domain.Groups$HibernateProxy$MmBQFksZ, Cannot call isReadOnlyBeforeAttachedToSession when isReadOnlySettingAvailable == true [cn.icedsoul.groupservice.domain.Groups#1]
```
查了一下应该是Hibernate内部策略有关，似乎有些麻烦，我先优化了以往写的查询：之前查询的方式是遍历所有id，一次一次去根据id查询groups，这样很不对，应该采用批量查询，这好像是自己第一次使用批量查询，使用方法也很简单：
```java
//主要在于关键词in，传入一个list即可
List<Groups> findByIdIn(List<Integer> ids);
```
修改之后又加上序列化的时间格式规范内容，重新运行尝试一下。

不行，问题还是出现了。

问题解决了，这涉及到了一个知识点：

在spring-data-jpa中，根据获取对象默认有两种方法，一种是getOne()一种findById，这两种其实是有区别的，getOne返回的是对象的引用，findById会返回一个Optional对象，可以通过get方法获取对象本身，因此只需要将getOne改为使用findById即可，用法参考如下：
```java
Optional<Groups> groupsOptional = groupRepository.findById(id);
if(groupsOptional.isPresent()){
      Groups groups = groupsOptional.get();
      return new Response(1, "获取用户群组成功", JSON.toJSONString(groups, SerializerFeature.UseSingleQuotes));
}
return new Response(0, "该群组不存在", null);
```
改成这样以后，即可正常序列化，以后使用getOne时需要注意这一点。

目前为止，群组聊天的查询已经完成，接下来就是完成群组聊天消息的转发和存储。

想要完成消息转发存储测试，首先需要完成邀请好友入群的功能。

试了一下，邀请好友加群的功能好像直接能用了。群组聊天的功能也是可以直接使用的，目前还差一个插入群组聊天消息的功能。

插入群组聊天记录的代码貌似之前写单人聊天室也顺手写了，解决了一个小bug之后发现。。问题出现查询群组聊天记录那里。。。再看看查询群组聊天记录哪里不对吧 = =

找到原因了，因为查询聊天记录最主要的是根据groupId查询，错写为id了。

修复这个Bug之后，还剩下几个小地方需要完成了：查看群组所有成员、对异常情况的处理。

查看群组所有成员的功能也基本完成了，异常情况留待以后综合测试时再说，接下来需要完成的是离线消息的处理。  

离线消息的处理主要是当用户不在线时，将发送给目标用户的消息暂时存储，留待之后用户上线时接受，主要分为两种情况：
1. 单人聊天消息，检查目标用户是否在线，如果不在线，将消息暂时存储至离线消息表。
2. 群组聊天消息，检查目标用户是否在线，如果不在线，将消息暂时存储至离线消息表。  
然后当用户上线时，检查离线消息表，如果有发送给自己的消息则接收消息。但是这里有一个问题：  
- 在目标用户未接受消息时，发送方查询聊天记录将无法看到自己发送的消息  
这毫无疑问是无法接受的，不管对方有没有收到，发送方都应该看到自己发送的消息，那么目前有三种方法：
- 先将消息存储至消息记录表，加个状态标记，离线消息表查询之后修改消息记录表状态而不是删除。
- 查询聊天记录时同时查询消息记录表和离线消息表。  
前者会造成消息冗余，但是仔细想想其实还好，理由如下：
- 完全可以不加状态标记，当做正常消息直接存储，只不过未接受的消息专门存储至离线消息表。
- 实现较为简单。
- 时间复杂读也较低。

同时，每个用户上线时都需要去检查自己的未接受消息，这是个很耗时的操作，是否可以给用户加一个标记，如果有离线消息则去查询，没有离线消息则直接跳过，这样可以减少没有离线消息的用户的登录后反应时间。

## 2019.06.20
今天准备做离线消息表，发现自己之前专门写了一个offline-message-service，里面把单人聊天消息和群组消息分开存放了，这固然有些好处，但是似乎也没太必要：因为我目前的做法是直接转发整条消息，也就是说服务端根据消息找到发送对象，然后把消息整个发送出去的，因为前台还需要对消息进行解析。那么不管是群组消息还是单人聊天消息都是转发整个消息，而且具体到单个发送对象之后不管是双人聊天消息还是群组聊天都是一样的。但是这样就会出现另一个问题：群组聊天消息如果拆开之后数量是非常大的，这样做会让离线消息表变得非常大，上线时的离线消息查询会很久。

但是群组聊天消息个人是否接受就是有这个麻烦。但是我有个新的想法：对于个人来说，群组聊天消息不像双人消息那样有明确的指向性，那么对于个人来说，群组发给自己的消息也不过是群组的聊天历史而已，那么似乎只要记住在用户离线时间，然后查询在这个时间段内哪几个群有新消息，然后去获取这段时间内的聊天消息就可以了。

仔细一想，貌似对双人聊天来说也是一样的，但是双人聊天和群组聊天不太一样的：群组聊天在不在线期间产生新消息的概率更大，而且数量较少，按照每个群去获取历史记录的做法比较合理，但是好友就不一样，好友数量会比较多，而且发送消息的数量也不会很多，这样使用离线消息表的方式专门存储似乎更为合理一点。

## 2019.07.06
今天增加了一个把数据库更改为共享数据库的版本，这样在小内存机器上也能够正常运行。  
然后在测试时发现了之前留下的一些Bug：
1. 创建群组时储存群组ID的地方写错了。
2. 群组离线消息转发时仍然存在问题。
第一个是写的逻辑问题，改一改，之后同步到主分支就可以了，第二个是toId的格式问题，修改起来比较麻烦，在此正好实现一下之前的想法：群组消息的转发并不转发具体消息内容，只是提一下某某群有新消息即可。  
OK，基本都改完了，明天有空部署一下～

## 2019.07.08
嘛，昨天浪了一天，今天继续准备测试一下把演示项目放上去，又发现了一些Bug，改的时候发现了一个有意思的内容：
添加好友关系：buildRelation的方法前面我加了一个@Transactional注解，在注册时发现了能成功在register远程调用这个接口，但是它在插入之后会紧接着回滚delete掉，尽管没有任何异常，我觉得既然是回滚，肯定是这个注解的锅。

emmm，事实证明和这个一点关系都没有2333，delete是我自己写的。真正出现问题的原因是user插入的时候并不是真正的插入，从控制台打印SQL的顺序就可以看出，insert是最后一句，但是实际上我很早就save了。所以要注意：   
** 在Spring Data Jpa中，调用save语句并不是真正的执行了insert语句，即便它会返回一个含有ID 的对象，貌似已经插入成功了，但是实际上到方法结束insert才被执行。想要立即执行insert语句应该使用saveAndFlush方法。 **  
所以这里的错误原因是我这里用于还没成功插入，然后去调用添加好友服务，添加好友服务又会回头去更新user的好友字段，但是这里还没有成功插入，那边update失败就报错了。  
目前大概就这样吧，搞一下服务器部署上去，更新一下说明文档，之后有时间列一下后续开发计划，比如说查看聊天记录（分页，查看所有聊天记录），还有表情，图片之类的。当然，最重要的是解决分布式聊天服务器寻找用户的问题，是session共享还是群发，总得搞定的，目前只是把之前的内容给还原完善了一下而已，真正困难的在后面，有空再详细描述吧。

## 2019.07.12
前两天花时间吧Kubernetes集群成功搭建了起来，也留下了必要的文档，之后可以简单整理一下，做成一个记录性质的博客。  
所以今天想要做的事情不是业务这一块儿，而是准备把WithMe部署至Kubernetes，目前我是使用Docker Compose进行部署的。在查找资料的过程中，我发现了官方的一个工具：Kompose，目的是帮助项目从docker compose迁移至kubernetes，具体安装方法很简单，直接在Github上搜索按照教程来即可。接下来开始尝试转换以及部署。

## 2019.07.12
过去几天花了不少时间，终于成功地把WithMe部署到了Kubernetes集群上并且成功运行了起来。  
首先，我使用Kompose把docker-compose文件转化为了一系列kubernetes部署文件，其实主要有以下几类：
1. PersistentVolumeClaim： 这类文件是对硬件资源的申请，在WithMe项目中主要是因为数据库的数据需要持久化，所以需要有pvc来申请存储资源。
2. Deployment： 这类部署文件是各个服务的核心部署文件，它定义了每个服务的必要参数、挂载卷、镜像地址等。
3. Service： 这类部署文件确定了各个服务的名称，暴露端口等。注意此处的NodePort、Port和TargetPort的区别。  
此处我首先遇到的问题是无法拉取镜像：DockerCompose可以现在本地build并拉取镜像，但是Kubernetes默认策略是从远程拉取镜像，因为我们事先无法预知某个服务会被部署到哪个节点，所以要么在所有节点都build好image，修改镜像拉取策略在本地。要么搭建docker镜像仓库或者上传至dockerhub。  
我采用的方式是上传服务至DockerHub仓库，首先需要在DockerHub注册账号，然后修改DockerCompose镜像名称前缀为自己的用户，在本地使用docker login登录DockerHub账号，之后使用docker-compose build 以及 docker-compose push就好了，如果报错请检查build的镜像名称前缀是否是自己的用户名。

镜像问题解决后，又遇到了新的问题：PVC是对存储资源的申请，但是我的集群并没有提供硬件资源。所以说pvc无法成功创建，那么mysql也会受影响无法正常部署。也就是说需要创建对应的PV，接下来我查阅了相关资料，学习了PV的创建方法，具体可以看PV&PVC，此处不再多说，我是用了Nfs来提供硬件资源，配置了nfs服务之后，创建对应的pv，pvc和mysql即可正常运行。

pv问题解决之后，还有一些小问题，比如后端服务端口无法访问，这是因为compose转化时的问题：在docker-compose文件中port有两种，一种是容器内端口，一种是映射至主机的端口，但是在Kubernetes集群中，端口一共有三种，一种是target port，这种是容器内端口，一种是port，这是暴露给集群内部其它服务的端口，还有一种是node port，这种是集群外部访问服务的端口。kompose的转换有些问题，把映射到主机的端口转化到了暴露给其它服务的端口，这导致服务之间无法访问，因此应该修改每个service的port参数，让其与代码中访问的端口保持一致。

此外还有一个问题，之前ws请求我没有使用nginx进行代理，直接访问主机暴露的端口的，但是这在kubernetes集群中显然不太合适，所以简单修改了nginx的conf文件，使nginx也代理ws的请求，这样整个集群就之暴露ui-service的一个端口其它都是集群内部通信。

到目前为止，WithMe3.0已经能够成功地部署在Kuernetes集群中，当然，与DockerCompose相比，部署过程无疑复杂了很多，但是起码做到了开发和部署分离，在不修改代码的前提下可以使用Docker Compose部署，也可以使用Kubernetes部署，接下来进一步实现CI和Devops也更加方便。  

之后我会写几篇博客，依次讲解Kuebrnetes集群的搭建，nfs服务器设置，pv&pvc，Deployment、Service，虽说有无数博客和书籍将这些东西已经讲得很好了，我还是想把自己摸索的记录和认识记录下来。之后会做的，到时再把链接更新在Readme吧。目前WithMe3.0部署暂时不会再继续向后探索，接下来需要继续进行业务上的开发。不过目前还是先做实验室这边的事情吧。

## 2019.08.09

今天继续进行业务上的开发。

今天主要准备实现的功能是在展示聊天记录时添加一个查看更久聊天记录的功能，不然为了优化查询性能只能查询30条聊天记录未免有些太少了。

不过发现了一个bug，之前我用nginx对websocket做了代理，使用k8s集群部署完全正常。不过今天使用docker-compose启动，发现docker-compose启动ws无法连接。

## 2019.08.12
今天继续修改之前的ws不能正常连接的bug，找了很久之后发现了问题：  
前端访问的不是容器，而是主机的端口，主机把流量转发至8081端口，然后nginx代理8081端口的流量，对其进行转发。所以在JS里面直接写连接8282，当时主机并没有对8282的端口进行转发，只有访问到了容器内部的8282端口，流量才会被nginx转发到ws服务器。  
所以目前采用的解决方法是，主机暴露两个端口，8282的转发至容器内8282.  
不过现在还有些问题：ws连接过一会儿就会自动断开，这个应该是nginx代理设置或者心跳问题，留待之后解决，加下来需要测试一下k8s集群是否存在代理异常的问题。

果然是有问题的，看来还得再改改。和docker-compose类似，暴露两个端口即可。

查了一下，一会儿就自动断开的机制果然是nginx的锅,nginx设置的访问超时时间为90s，但是ws时间应该比较长。所以在nginx的配置文件中设置一下proxy_read_timeout即可解决这个问题。

这两天时间几乎没有做新的东西，不过也算解决了一些遗留问题，也还行吧。接下来要一边写博客一遍继续开发啦～不能一直摸鱼摸下去。
